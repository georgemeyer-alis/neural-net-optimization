# Optimization for Deep Learning

This repository will contain implementations of various popular optimization algorithms for deep learning, including SGD, Adam, AdamW and RAdam. Work in progress!

_____

## Related papers

Material in this repository has been developed as part of a special course / study. This is the tentative list of papers that we discuss:

[An Overview of Gradient Descent Optimization Algorithms](https://arxiv.org/abs/1609.04747)

[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)

[On the Convergence of Adam and Beyond](https://arxiv.org/abs/1904.09237)

[Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101)

[On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265v1)
